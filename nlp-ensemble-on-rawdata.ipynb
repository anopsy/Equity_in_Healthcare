{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":65862,"databundleVersionId":7469115,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-19T16:16:18.943079Z","iopub.execute_input":"2024-02-19T16:16:18.943491Z","iopub.status.idle":"2024-02-19T16:16:19.298214Z","shell.execute_reply.started":"2024-02-19T16:16:18.943454Z","shell.execute_reply":"2024-02-19T16:16:19.297186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/widsdatathon2024-challenge1/test.csv')\ntrain = pd.read_csv('/kaggle/input/widsdatathon2024-challenge1/training.csv')","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:19.300190Z","iopub.execute_input":"2024-02-19T16:16:19.300572Z","iopub.status.idle":"2024-02-19T16:16:19.726713Z","shell.execute_reply.started":"2024-02-19T16:16:19.300547Z","shell.execute_reply":"2024-02-19T16:16:19.724999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# In this notebook I'll implement NLP on the description column of the dataset.\nFirst I drop the columns that contain too many NaN's ","metadata":{}},{"cell_type":"code","source":"train.drop(columns = ['metastatic_first_novel_treatment', 'metastatic_first_novel_treatment_type', 'patient_race', 'payer_type' , 'bmi', 'female', 'male'], inplace= True)\ntest.drop(columns = ['metastatic_first_novel_treatment', 'metastatic_first_novel_treatment_type', 'patient_race', 'payer_type' , 'bmi', 'female', 'male'], inplace= True)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:19.727832Z","iopub.execute_input":"2024-02-19T16:16:19.728803Z","iopub.status.idle":"2024-02-19T16:16:19.751890Z","shell.execute_reply.started":"2024-02-19T16:16:19.728767Z","shell.execute_reply":"2024-02-19T16:16:19.751223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# I concatenate train and test dataset so I can perform tokenization on all of the data","metadata":{}},{"cell_type":"code","source":"whole_df = pd.concat([train.drop(columns=['DiagPeriodL90D']), test])","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:19.753232Z","iopub.execute_input":"2024-02-19T16:16:19.754417Z","iopub.status.idle":"2024-02-19T16:16:19.767030Z","shell.execute_reply.started":"2024-02-19T16:16:19.754370Z","shell.execute_reply":"2024-02-19T16:16:19.765463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:19.770290Z","iopub.execute_input":"2024-02-19T16:16:19.770695Z","iopub.status.idle":"2024-02-19T16:16:20.997911Z","shell.execute_reply.started":"2024-02-19T16:16:19.770659Z","shell.execute_reply":"2024-02-19T16:16:20.996807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# I split the words in the description and lowercase them, I remove stopwords and replace some of shortened words","metadata":{}},{"cell_type":"code","source":"\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words('english'))\n\n# Convert to lower case and remove stop words\nwhole_df['processed_description'] = whole_df['breast_cancer_diagnosis_desc'].apply(lambda x: ' '.join(word for word in x.lower().split() if word not in stop_words))\n\n# Define your custom mapping\nmapping_dict = {\n    'malig': 'malignant',\n    'unsp': 'unspecified',\n    'ovrlp': 'overlapping',\n    'neoplm' : 'neoplasm',\n    \n}\n\n# Function to replace words based on the mapping\ndef replace_words(text):\n    return ' '.join(mapping_dict.get(word, word) for word in text.split())\n\n# Apply the function to the 'processed_description' column\nwhole_df['processed_description'] = whole_df['processed_description'].apply(replace_words)\n\n# Now you can continue with the tokenization as before\nwhole_df['tokenized_description'] = whole_df['processed_description'].apply(nltk.word_tokenize)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:20.999406Z","iopub.execute_input":"2024-02-19T16:16:20.999751Z","iopub.status.idle":"2024-02-19T16:16:22.869211Z","shell.execute_reply.started":"2024-02-19T16:16:20.999720Z","shell.execute_reply":"2024-02-19T16:16:22.867669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"whole_df['tokenized_description']","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:22.871038Z","iopub.execute_input":"2024-02-19T16:16:22.871418Z","iopub.status.idle":"2024-02-19T16:16:22.883991Z","shell.execute_reply.started":"2024-02-19T16:16:22.871391Z","shell.execute_reply":"2024-02-19T16:16:22.882043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Join the tokens back into a single string\nwhole_df['tokenized_description'] = whole_df['tokenized_description'].str.join(' ')\n\n# Initialize the TfidfVectorizer\ntfidf = TfidfVectorizer()\n\n# Fit and transform the 'tokenized_description' column\ntfidf_matrix = tfidf.fit_transform(whole_df['tokenized_description'])\n\n# Convert the matrix into a DataFrame\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n\n# Now tfidf_df is a DataFrame where each word is a separate column and the value is its TF-IDF score","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:22.885767Z","iopub.execute_input":"2024-02-19T16:16:22.886937Z","iopub.status.idle":"2024-02-19T16:16:23.052287Z","shell.execute_reply.started":"2024-02-19T16:16:22.886895Z","shell.execute_reply":"2024-02-19T16:16:23.051303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:23.053333Z","iopub.execute_input":"2024-02-19T16:16:23.053604Z","iopub.status.idle":"2024-02-19T16:16:23.083789Z","shell.execute_reply.started":"2024-02-19T16:16:23.053582Z","shell.execute_reply":"2024-02-19T16:16:23.082480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"whole_df.reset_index(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:23.085497Z","iopub.execute_input":"2024-02-19T16:16:23.085792Z","iopub.status.idle":"2024-02-19T16:16:23.091235Z","shell.execute_reply.started":"2024-02-19T16:16:23.085772Z","shell.execute_reply":"2024-02-19T16:16:23.090354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# I combine the tokenized dataframe with my original dataframe","metadata":{}},{"cell_type":"code","source":"tokenized_df = pd.concat([whole_df, tfidf_df], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:23.092407Z","iopub.execute_input":"2024-02-19T16:16:23.093748Z","iopub.status.idle":"2024-02-19T16:16:23.115661Z","shell.execute_reply.started":"2024-02-19T16:16:23.093719Z","shell.execute_reply":"2024-02-19T16:16:23.113837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_df_types = tokenized_df.dtypes.to_dict()\n\nfor key in tokenized_df_types:\n    if tokenized_df_types[key] =='O':\n        tokenized_df[key] = tokenized_df[key].astype('category')","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:23.117712Z","iopub.execute_input":"2024-02-19T16:16:23.118136Z","iopub.status.idle":"2024-02-19T16:16:23.150537Z","shell.execute_reply.started":"2024-02-19T16:16:23.118100Z","shell.execute_reply":"2024-02-19T16:16:23.149859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_df['patient_zip3']=tokenized_df['patient_zip3'].astype('category')\ntokenized_df.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:23.151436Z","iopub.execute_input":"2024-02-19T16:16:23.152400Z","iopub.status.idle":"2024-02-19T16:16:23.160075Z","shell.execute_reply.started":"2024-02-19T16:16:23.152378Z","shell.execute_reply":"2024-02-19T16:16:23.159211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_col = tokenized_df.select_dtypes(exclude=['category']).columns\ncat_col = tokenized_df.select_dtypes(include=['category']).columns\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:23.164373Z","iopub.execute_input":"2024-02-19T16:16:23.164716Z","iopub.status.idle":"2024-02-19T16:16:23.180841Z","shell.execute_reply.started":"2024-02-19T16:16:23.164695Z","shell.execute_reply":"2024-02-19T16:16:23.179121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cat_col)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:23.182086Z","iopub.execute_input":"2024-02-19T16:16:23.183546Z","iopub.status.idle":"2024-02-19T16:16:23.188666Z","shell.execute_reply.started":"2024-02-19T16:16:23.183510Z","shell.execute_reply":"2024-02-19T16:16:23.187782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(num_col)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:23.189685Z","iopub.execute_input":"2024-02-19T16:16:23.190865Z","iopub.status.idle":"2024-02-19T16:16:23.200357Z","shell.execute_reply.started":"2024-02-19T16:16:23.190835Z","shell.execute_reply":"2024-02-19T16:16:23.199424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# In my previous notebooks I used OneHotEncoding for categorical data, this time I want to check the OrdinalEncoder, since I saw somebody use it. I was curious about the results, since the categorical data are not of ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\n\n# Initialize the encoder\nencoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n# Loop through each categorical column\nfor col in cat_col:\n    # Fit the encoder on the training data\n    encoder.fit(tokenized_df[[col]])\n\n    # Transform both training and test data\n    tokenized_df[col] = encoder.transform(tokenized_df[[col]])","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:23.201656Z","iopub.execute_input":"2024-02-19T16:16:23.202058Z","iopub.status.idle":"2024-02-19T16:16:23.276964Z","shell.execute_reply.started":"2024-02-19T16:16:23.202022Z","shell.execute_reply":"2024-02-19T16:16:23.275818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# I split the dataframe again in train and test sets","metadata":{}},{"cell_type":"code","source":"train_df = tokenized_df[:12906]\ntest_df = tokenized_df[12906:]","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:23.279161Z","iopub.execute_input":"2024-02-19T16:16:23.279480Z","iopub.status.idle":"2024-02-19T16:16:23.285496Z","shell.execute_reply.started":"2024-02-19T16:16:23.279455Z","shell.execute_reply":"2024-02-19T16:16:23.283995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['DiagPeriodL90D']=train['DiagPeriodL90D'].astype('category')\ntrain_df.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:23.287159Z","iopub.execute_input":"2024-02-19T16:16:23.287405Z","iopub.status.idle":"2024-02-19T16:16:23.301749Z","shell.execute_reply.started":"2024-02-19T16:16:23.287383Z","shell.execute_reply":"2024-02-19T16:16:23.300514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.columns","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:23.302583Z","iopub.execute_input":"2024-02-19T16:16:23.302894Z","iopub.status.idle":"2024-02-19T16:16:23.309088Z","shell.execute_reply.started":"2024-02-19T16:16:23.302848Z","shell.execute_reply":"2024-02-19T16:16:23.308193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import StackingClassifier\n\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:23.310923Z","iopub.execute_input":"2024-02-19T16:16:23.311384Z","iopub.status.idle":"2024-02-19T16:16:25.258149Z","shell.execute_reply.started":"2024-02-19T16:16:23.311352Z","shell.execute_reply":"2024-02-19T16:16:25.256910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:25.259583Z","iopub.execute_input":"2024-02-19T16:16:25.260125Z","iopub.status.idle":"2024-02-19T16:16:25.265125Z","shell.execute_reply.started":"2024-02-19T16:16:25.260094Z","shell.execute_reply":"2024-02-19T16:16:25.264356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\nX = train_df.drop(columns=['index', 'patient_id','DiagPeriodL90D', 'patient_gender' ])\ny = le.fit_transform(train_df['DiagPeriodL90D'])","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:25.266364Z","iopub.execute_input":"2024-02-19T16:16:25.266866Z","iopub.status.idle":"2024-02-19T16:16:25.282726Z","shell.execute_reply.started":"2024-02-19T16:16:25.266837Z","shell.execute_reply":"2024-02-19T16:16:25.281674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, stratify=y, shuffle=True, random_state = 2137)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:25.283852Z","iopub.execute_input":"2024-02-19T16:16:25.284727Z","iopub.status.idle":"2024-02-19T16:16:25.306506Z","shell.execute_reply.started":"2024-02-19T16:16:25.284695Z","shell.execute_reply":"2024-02-19T16:16:25.305362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# I set parameters for the estimators in my ensemble","metadata":{}},{"cell_type":"code","source":"params_xgb = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0, 0.5, 1],\n        'subsample': [0.6, 0.8],\n        'colsample_bytree': [0.6, 0.8],\n        'max_depth': [3, 4, 5],\n        'learning_rate' : [0.001, 0.01,  0.05],\n        'n_estimators' : [500, 1000, 2000],\n        'scale_pos_weight' : [0.6]\n        }\n\nparams_lgb = {\n        'bagging_fraction': [0.5, 0.8],\n        'bagging_freq': [3, 5, 8],\n        'feature_fraction': [0.5, 0.8],\n        'max_depth': [8, 10, 13],\n        'min_data_in_leaf': [60, 90, 120],\n        'num_leaves': [100, 1200, 1550]   \n}\n\nparams_cat = {\n        'iterations': [100, 200, 400],\n        'learning_rate': [0.01, 0.1 , 0.5],\n        'depth': [4, 5, 6],    \n        'min_data_in_leaf' : [50, 100, 150, 200]\n}\n\nparams_ada = {\n        #'max_depth': [3, 4, 5],\n        'learning_rate' : [0.001, 0.01, 0.02, 0.05],\n        'n_estimators' : [500, 1000, 1500]\n    \n}","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:25.307648Z","iopub.execute_input":"2024-02-19T16:16:25.308070Z","iopub.status.idle":"2024-02-19T16:16:25.315735Z","shell.execute_reply.started":"2024-02-19T16:16:25.308044Z","shell.execute_reply":"2024-02-19T16:16:25.313681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:25.317634Z","iopub.execute_input":"2024-02-19T16:16:25.318273Z","iopub.status.idle":"2024-02-19T16:16:25.337745Z","shell.execute_reply.started":"2024-02-19T16:16:25.318240Z","shell.execute_reply":"2024-02-19T16:16:25.336349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=3, shuffle = True, random_state = 2137)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:25.339127Z","iopub.execute_input":"2024-02-19T16:16:25.339432Z","iopub.status.idle":"2024-02-19T16:16:25.346854Z","shell.execute_reply.started":"2024-02-19T16:16:25.339407Z","shell.execute_reply":"2024-02-19T16:16:25.345964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tune_xgb = xgb.XGBClassifier(objective='binary:logistic', enable_categorical=True)\nrand_xgb = RandomizedSearchCV(tune_xgb, param_distributions=params_xgb, n_iter=40, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train,y_train), verbose=1, random_state=2137)\nrand_xgb.fit(X_train, y_train)\nrand_xgb_pred = rand_xgb.predict_proba(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:16:25.347807Z","iopub.execute_input":"2024-02-19T16:16:25.348490Z","iopub.status.idle":"2024-02-19T16:21:26.372465Z","shell.execute_reply.started":"2024-02-19T16:16:25.348465Z","shell.execute_reply":"2024-02-19T16:21:26.371604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(roc_auc_score(y_test, rand_xgb_pred[:, 1]))\nprint(confusion_matrix(y_test, np.round(rand_xgb_pred[:, 1])))","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:21:26.373729Z","iopub.execute_input":"2024-02-19T16:21:26.374363Z","iopub.status.idle":"2024-02-19T16:21:26.384673Z","shell.execute_reply.started":"2024-02-19T16:21:26.374337Z","shell.execute_reply":"2024-02-19T16:21:26.383699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best = rand_xgb.best_estimator_\nbest_features = best.feature_importances_\nfor idx, feat in enumerate(X.columns):\n    print(feat, best_features[idx])","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:21:26.386177Z","iopub.execute_input":"2024-02-19T16:21:26.386492Z","iopub.status.idle":"2024-02-19T16:21:26.398003Z","shell.execute_reply.started":"2024-02-19T16:21:26.386466Z","shell.execute_reply":"2024-02-19T16:21:26.396940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tune_lgb = lgb.LGBMClassifier()\nrand_lgb = RandomizedSearchCV(tune_lgb, param_distributions=params_lgb, n_iter=40, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train,y_train), verbose=1, random_state = 2137 )\nrand_lgb.fit(X_train, y_train)\nrand_lgb_pred = rand_lgb.predict_proba(X_test)\nprint(roc_auc_score(y_test, rand_lgb_pred[:, 1]))\nprint(confusion_matrix(y_test, np.round(rand_lgb_pred[:, 1])))","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:21:26.399039Z","iopub.execute_input":"2024-02-19T16:21:26.399508Z","iopub.status.idle":"2024-02-19T16:23:27.767018Z","shell.execute_reply.started":"2024-02-19T16:21:26.399482Z","shell.execute_reply":"2024-02-19T16:23:27.765453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tune_cat = cb.CatBoostClassifier()\nrand_cat = RandomizedSearchCV(tune_cat, param_distributions=params_cat, n_iter=40, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train,y_train), verbose=1, random_state = 2137 )\nrand_cat.fit(X_train, y_train)\nrand_cat_pred = rand_cat.predict_proba(X_test)\nprint(roc_auc_score(y_test, rand_cat_pred[:, 1]))\nprint(confusion_matrix(y_test, np.round(rand_cat_pred[:, 1])))","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:23:27.768272Z","iopub.execute_input":"2024-02-19T16:23:27.768526Z","iopub.status.idle":"2024-02-19T16:27:08.600729Z","shell.execute_reply.started":"2024-02-19T16:23:27.768503Z","shell.execute_reply":"2024-02-19T16:27:08.599340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tune_ada = AdaBoostClassifier()\n#rand_ada = RandomizedSearchCV(tune_ada, param_distributions=params_ada, n_iter=40, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train,y_train), verbose=1, random_state = 2137 )\n#rand_ada.fit(X_train, y_train)\n#rand_ada_pred = rand_cat.predict_proba(X_test)\n#print(roc_auc_score(y_test, rand_ada_pred[:, 1]))\n#print(confusion_matrix(y_test, np.round(rand_ada_pred[:, 1])))","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:27:08.602111Z","iopub.execute_input":"2024-02-19T16:27:08.602436Z","iopub.status.idle":"2024-02-19T16:27:08.609542Z","shell.execute_reply.started":"2024-02-19T16:27:08.602406Z","shell.execute_reply":"2024-02-19T16:27:08.608256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# I use 3 estimators (xgb, lgb and catboost) and xgb as my meta estimator","metadata":{}},{"cell_type":"code","source":"clf_xgb = xgb.XGBClassifier(**rand_xgb.best_params_,enable_categorical =  True, objective = 'binary:logistic')\nclf_lgb = lgb.LGBMClassifier(**rand_lgb.best_params_)\nclf_cat = cb.CatBoostClassifier(**rand_cat.best_params_)\n#clf_ada = AdaBoostClassifier(**rand_ada.best_params_)\nclf_lr = LogisticRegression(class_weight = 'balanced', random_state = 2137)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:27:08.612143Z","iopub.execute_input":"2024-02-19T16:27:08.612464Z","iopub.status.idle":"2024-02-19T16:27:08.622093Z","shell.execute_reply.started":"2024-02-19T16:27:08.612439Z","shell.execute_reply":"2024-02-19T16:27:08.620688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_voting = VotingClassifier(\n    estimators=[\n        ('xgb', clf_xgb),\n        ('lgb', clf_lgb),\n        ('cat', clf_cat)],\n    voting = 'soft',\n    verbose=False\n    \n)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:27:08.625593Z","iopub.execute_input":"2024-02-19T16:27:08.625956Z","iopub.status.idle":"2024-02-19T16:27:08.633766Z","shell.execute_reply.started":"2024-02-19T16:27:08.625927Z","shell.execute_reply":"2024-02-19T16:27:08.632263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_voting.fit(X_train, y_train)\ny_pred = clf_voting.predict_proba(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:27:08.634797Z","iopub.execute_input":"2024-02-19T16:27:08.635088Z","iopub.status.idle":"2024-02-19T16:27:23.937278Z","shell.execute_reply.started":"2024-02-19T16:27:08.635062Z","shell.execute_reply":"2024-02-19T16:27:23.936371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(roc_auc_score(y_test, y_pred[:, 1]))","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:28:55.795855Z","iopub.execute_input":"2024-02-19T16:28:55.796236Z","iopub.status.idle":"2024-02-19T16:28:55.805705Z","shell.execute_reply.started":"2024-02-19T16:28:55.796205Z","shell.execute_reply":"2024-02-19T16:28:55.804373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_proba = clf_voting.predict_proba(test_df.drop(columns=['patient_gender','index', 'patient_id']))\ntest['DiagPeriodL90D'] = sub_proba[:, 1]\nsub = test[['patient_id','DiagPeriodL90D']]\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:32:20.343473Z","iopub.execute_input":"2024-02-19T16:32:20.343812Z","iopub.status.idle":"2024-02-19T16:32:20.555277Z","shell.execute_reply.started":"2024-02-19T16:32:20.343785Z","shell.execute_reply":"2024-02-19T16:32:20.554282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T16:32:26.115352Z","iopub.execute_input":"2024-02-19T16:32:26.115662Z","iopub.status.idle":"2024-02-19T16:32:26.132684Z","shell.execute_reply.started":"2024-02-19T16:32:26.115641Z","shell.execute_reply":"2024-02-19T16:32:26.131497Z"},"trusted":true},"execution_count":null,"outputs":[]}]}